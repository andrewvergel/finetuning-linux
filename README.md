# Fine-tuning LoRA en Ubuntu + RTX 4060â€¯Ti

> Proyecto personal documentado como parte de mi portafolio. Implementa un flujo completo de fine-tuning con LoRA sobre DialoGPT usando CUDA en Ubuntu, incluyendo preparaciÃ³n del entorno, entrenamiento reproducible y despliegue de inferencia.

## âœ¨ Resumen del Proyecto
- **Objetivo:** entrenar un chatbot corporativo capaz de responder procedimientos internos a partir de un dataset de instrucciones propio.
- **TecnologÃ­as:** Python, PyTorch 2.8, Transformers 4.35, TRL 0.7, LoRA (PEFT), CUDA 12.1.
- **Hardware:** NVIDIA RTX 4060â€¯Ti (16â€¯GB VRAM) en Ubuntu 22.04.
- **Repositorio:** [`andrewvergel/finetuning-linux`](https://github.com/andrewvergel/finetuning-linux)

## ðŸ§± Arquitectura y Flujo
1. **Bootstrap de entorno** â€“ instalaciÃ³n de drivers, CUDA y dependencias en un equipo limpio.
2. **Dataset JSONL** â€“ prompts internos versionados en `data/instructions.jsonl`.
3. **Script de entrenamiento** â€“ `scripts/finetune_lora.py` (v1.0.9) realiza duplicaciÃ³n inteligente del dataset y ajusta hiperparÃ¡metros para escenarios de pocos datos.
4. **Inferencia controlada** â€“ `scripts/inference_lora.py` (v1.0.2) con decodificaciÃ³n determinista para evaluar resultados.
5. **Reportes** â€“ se genera `training_info.json` con metadatos del entrenamiento.

## ðŸš€ Puesta en Marcha desde Cero
```bash
# 1. Clonar el repositorio
git clone https://github.com/andrewvergel/finetuning-linux.git
cd finetuning-linux

# 2. Crear y activar entorno virtual
python3 -m venv .venv
source .venv/bin/activate

# 3. Actualizar pip e instalar dependencias
pip install --upgrade pip setuptools wheel
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install -r requirements.txt
```
> ðŸ’¡ Si partes de un servidor reciÃ©n formateado, instala previamente los drivers NVIDIA, CUDA 12.1 y utilidades del sistema (detallado en secciones posteriores del repositorio original).
> ðŸ“¦ El archivo `requirements.txt` (incluido en el repo) contiene todas las librerÃ­as auxiliares requeridas para el proyecto.
> ðŸ” Antes de entrenar, puedes ejecutar `python scripts/validate_environment.py` para verificar versiones de Python, CUDA, VRAM disponible, dataset y dependencias.

## ðŸ“š Dataset de Instrucciones
```bash
mkdir -p data
cat > data/instructions.jsonl << 'JSONL'
{"system":"Eres un asistente experto en procesos internos.","input":"Dame los pasos para conciliar pagos de los lunes.","output":"1) Exporta el CSV del banco.\n2) Ejecuta el job 'reconcile_monday'.\n3) Revisa discrepancias en la tabla 'recon_issues'."}
{"system":"Habla en tono profesional y conciso.","input":"Resume este procedimiento en tres bullets.","output":"â€¢ Exportar CSV.\nâ€¢ Ejecutar job.\nâ€¢ Validar discrepancias."}
{"system":"Responde siempre con pasos numerados.","input":"Â¿CÃ³mo abro un ticket de soporte?","output":"1) Entra al helpdesk.\n2) Crea ticket 'Incidente'.\n3) Adjunta evidencias."}
JSONL
```
> â„¹ï¸ `data/instructions.jsonl` **ya viene versionado en este repositorio** y es el Ãºnico archivo permitido dentro de `data/`. El script de entrenamiento duplica automÃ¡ticamente el dataset si detecta menos de 200 muestras, pero se recomienda ampliarlo manualmente con mÃ¡s casuÃ­sticas para mejorar la diversidad de respuestas.

## ðŸ› ï¸ Script de Entrenamiento (`scripts/finetune_lora.py`)
- Basado en LoRA con rank 32 sobre las capas `c_attn` y `c_proj` de DialoGPT-medium.
- Duplica datasets pequeÃ±os para asegurar convergencia.
- HiperparÃ¡metros ajustados a escenarios low-data: batch 2, 30 Ã©pocas, scheduler `cosine`, warmup 5%.
- Genera un `training_info.json` con mÃ©tricas bÃ¡sicas y contexto de hardware.

## ðŸ’¬ Script de Inferencia (`scripts/inference_lora.py`)
- Carga el adaptador LoRA desde `models/out-tinyllama-lora`.
- Usa decodificaciÃ³n determinista (sin muestreo) para validar fÃ¡cilmente regresiones.
- Incluye loop interactivo opcional y estadÃ­sticas de uso de GPU.

## â–¶ï¸ EjecuciÃ³n
```bash
# Entrenamiento
python scripts/finetune_lora.py

# Inferencia inicial
python scripts/inference_lora.py
```
> El entrenamiento guarda pesos LoRA en `models/out-tinyllama-lora`. Puedes fusionarlos con el modelo base usando `scripts/merge_adapter.py` si necesitas un Ãºnico checkpoint.

## ðŸ§© Estructura del Proyecto
```
finetuning-linux/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ instructions.jsonl           # Dataset versionado
â”œâ”€â”€ models/                          # Salidas de entrenamiento (ignorado en git)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ finetune_lora.py             # Entrenamiento LoRA (v1.0.9)
â”‚   â”œâ”€â”€ inference_lora.py            # Inferencia determinista (v1.0.2)
â”‚   â””â”€â”€ validate_environment.py      # Checklist opcional de diagnÃ³stico
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md (este documento)
```

## ðŸ§ª Resultados Destacados
- Con sÃ³lo 20 instrucciones iniciales se logra un chatbot que entiende flujos corporativos simples.
- El adaptador LoRA replica procedimientos secuenciales con numeraciones consistentes.
- El tiempo de entrenamiento en RTX 4060â€¯Ti < 10 minutos.

## ðŸ›£ï¸ PrÃ³ximos Pasos
- Aumentar el dataset con mÃ¡s procesos internos.
- AÃ±adir evaluaciÃ³n cuantitativa (BLEU, ROUGE, precisiÃ³n manual).
- Integrar despliegue vÃ­a API REST para consumir el modelo fine-tuned en producciÃ³n.

## ðŸ“„ Licencia y Autor
Este proyecto es de uso personal y demuestra capacidades de MLOps / IA aplicada. Puedes reutilizarlo adaptando los scripts a tus propios datos.

**Autor:** Andrew Vergel  Â·  [LinkedIn](https://www.linkedin.com/in/andrewvergel/)  Â·  [Repositorio GitHub](https://github.com/andrewvergel/finetuning-linux)

Â¡Gracias por revisar este proyecto! Estoy abierto a colaborar en iniciativas de IA aplicada, automatizaciÃ³n de procesos y plataformas de asistentes inteligentes.

## â˜ï¸ Infraestructura Recomendada
Para quienes no cuenten con una RTX 4060â€¯Ti local, recomiendo utilizar instancias bajo demanda en [cloud.vast.ai](https://cloud.vast.ai/instances/). Las pruebas finales de este proyecto se realizaron en la instancia `27712045` (host `79466`, machine `13313`) con las siguientes caracterÃ­sticas:

- **GPU:** 16â€¯GB VRAM (CUDA 12.9, ~21.6 TFLOPS)
- **CPU:** AMD Ryzen 9 3900X (12/24 hilos)
- **RAM:** 64â€¯GB DDR4
- **Almacenamiento:** NVMe PCIe 4.0 (4â€¯TB, ~4.7â€¯GB/s)
- **Red:** ~1.6â€¯Gbps simÃ©tricos

La plataforma ofrece una buena relaciÃ³n costo/rendimiento (â‰ˆ236 DLP/$/hr) y permite desplegar rÃ¡pidamente el entorno descrito en este repositorio.
