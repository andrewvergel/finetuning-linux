# Fine-tuning LoRA en Ubuntu + RTX 4060â€¯Ti

> Proyecto personal documentado como parte de mi portafolio. Implementa un flujo completo de fine-tuning con LoRA sobre DialoGPT usando CUDA en Ubuntu, incluyendo preparaciÃ³n del entorno, entrenamiento reproducible y despliegue de inferencia.

## âœ¨ Resumen del Proyecto
- **Objetivo:** entrenar un chatbot corporativo capaz de responder procedimientos internos a partir de un dataset de instrucciones propio.
- **TecnologÃ­as:** Python, PyTorch 2.8, Transformers 4.35, TRL 0.7, LoRA (PEFT), CUDA 12.1.
- **Hardware:** NVIDIA RTX 4060â€¯Ti (16â€¯GB VRAM) en Ubuntu 22.04.
- **Repositorio:** [`andrewvergel/finetuning-linux`](https://github.com/andrewvergel/finetuning-linux)

## ðŸ§± Arquitectura y Flujo
1. **Bootstrap de entorno** â€“ instalaciÃ³n de drivers, CUDA y dependencias en un equipo limpio.
2. **Dataset JSONL** â€“ prompts internos versionados en `data/instructions.jsonl`.
3. **Script de entrenamiento** â€“ `scripts/finetune_lora.py` (v1.1.1) realiza duplicaciÃ³n inteligente del dataset y ajusta hiperparÃ¡metros para escenarios de pocos datos.
4. **Inferencia controlada** â€“ `scripts/inference_lora.py` (v1.0.2) con decodificaciÃ³n determinista para evaluar resultados.
5. **Reportes** â€“ se genera `training_info.json` con metadatos del entrenamiento.

## ðŸš€ Puesta en Marcha desde Cero
```bash
# 1. Clonar el repositorio
git clone https://github.com/andrewvergel/finetuning-linux.git
cd finetuning-linux

# 2. Crear y activar entorno virtual
python3 -m venv .venv
source .venv/bin/activate

# 3. Actualizar pip e instalar dependencias
pip install --upgrade pip setuptools wheel
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install "numpy<2.0" pyarrow==14.0.1
pip install -r requirements.txt
```
> ðŸ’¡ Si partes de un servidor reciÃ©n formateado, instala previamente los drivers NVIDIA, CUDA 12.1 y utilidades del sistema (detallado en secciones posteriores del repositorio original).
> ðŸ“¦ `requirements.txt` incluye todas las librerÃ­as auxiliares; aun asÃ­, instalamos `numpy<2.0` y `pyarrow==14.0.1` antes para evitar conflictos conocidos con `datasets` (error `PyExtensionType`).
> ðŸ” Antes de entrenar, puedes ejecutar `python scripts/validate_environment.py` para verificar versiones de Python, CUDA, VRAM disponible, dataset y dependencias.
> ðŸ§¾ Cada entrenamiento deja un log detallado en `logs/debug_last_run.log` con mÃ©tricas y respuestas de validaciÃ³n automÃ¡tica.

## ðŸ“š Dataset de Instrucciones
```bash
mkdir -p data
cat > data/instructions.jsonl << 'JSONL'
{"system":"Eres un asistente experto en procesos internos.","input":"Dame los pasos para conciliar pagos de los lunes.","output":"1) Exporta el CSV del banco.\n2) Ejecuta el job 'reconcile_monday'.\n3) Revisa discrepancias en la tabla 'recon_issues'."}
{"system":"Habla en tono profesional y conciso.","input":"Resume este procedimiento en tres bullets.","output":"â€¢ Exportar CSV.\nâ€¢ Ejecutar job.\nâ€¢ Validar discrepancias."}
{"system":"Responde siempre con pasos numerados.","input":"Â¿CÃ³mo abro un ticket de soporte?","output":"1) Entra al helpdesk.\n2) Crea ticket 'Incidente'.\n3) Adjunta evidencias."}
JSONL
```
> â„¹ï¸ `data/instructions.jsonl` **ya viene versionado en este repositorio** y es el Ãºnico archivo permitido dentro de `data/`. El script de entrenamiento duplica automÃ¡ticamente el dataset si detecta menos de 200 muestras, pero se recomienda ampliarlo manualmente con mÃ¡s casuÃ­sticas para mejorar la diversidad de respuestas.

## ðŸ› ï¸ Script de Entrenamiento (`scripts/finetune_lora.py`)
- Basado en LoRA (r=32) sobre las capas `c_attn` y `c_proj` de DialoGPT-medium (ajustable por constantes).
- El entrenamiento usa por defecto `data/instructions.jsonl` (puedes sobreescribirlo con la variable `FINETUNE_DATA_PATH`).
- Duplica datasets pequeÃ±os hasta ~420 ejemplos solo sobre el split de entrenamiento.
- Entrenamiento altamente regularizado: batch efectivo 8 (1Ã—8), 18 Ã©pocas, scheduler `cosine` (warmup 5%) y sin weight decay.
- Genera `training_info.json` con metadatos y deja un log detallado en `logs/debug_last_run.log`.
- Reserva automÃ¡ticamente 15% para validaciÃ³n, corre evaluaciÃ³n al final de cada Ã©poca y guarda el mejor checkpoint segÃºn `eval_loss`.
- Ejecuta una evaluaciÃ³n rÃ¡pida al final tomando 12 ejemplos del split de validaciÃ³n (o un fallback predefinido) y deja la comparaciÃ³n esperada/obtenida en el log.

### ðŸ” InterpretaciÃ³n de logs y tuning
#### PÃ©rdida de entrenamiento
- Valores de `loss` entre **4.0â€“5.0** son tÃ­picos para DialoGPT con datasets repetidos. Si cae de ~5.2 a ~4.1 en pocas Ã©pocas, la convergencia va bien. Si la pÃ©rdida se estanca >3.8 tras 20 Ã©pocas, considera subir `LEARNING_RATE` o reducir `LORA_DROPOUT`.
#### Learning rate efectivo
- Con `LEARNING_RATE = 4e-5` deberÃ­as ver valores ~3.9e-05 a 4.0e-05 en los logs. Si cae demasiado rÃ¡pido (<3e-05) en las primeras Ã©pocas, sube `WARMUP_RATIO`.
#### SeÃ±ales de overfitting
- PÃ©rdida de entrenamiento baja, pero validaciÃ³n no mejora o sube â†’ sube `LORA_DROPOUT` o baja `NUM_EPOCHS`.
#### Si el modelo delira
- Repite frases, respuestas circulares o incoherentes â†’ sube `LORA_DROPOUT` a 0.2, baja `NUM_EPOCHS` a 20, y aÃ±ade mÃ¡s ejemplos Ãºnicos al dataset.

#### Fases avanzadas del entrenamiento (Ã©pocas 20+)
- Si ves `loss` ~2.0 estable y `learning_rate` ~3e-06, el modelo estÃ¡ cerca del mÃ­nimo. 
- Continuar entrenando puede llevar a **overfitting sutil** (responde mejor a ejemplos de entrenamiento pero falla en variaciones). 
- **Criterio de parada:** si `eval_loss` deja de bajar por 3â€“4 Ã©pocas seguidas, detÃ©n el entrenamiento. 
- **Si necesitas mÃ¡s calidad:** en lugar de mÃ¡s Ã©pocas, amplÃ­a el dataset real (no repitas) o prueba un modelo base mayor.

### ðŸ“Š GuÃ­a rÃ¡pida de hiperparÃ¡metros
- `DATASET_MIN_EXAMPLES = 160` â†’ nÃºmero mÃ­nimo de muestras tras repetir el split de entrenamiento (ej.: con 20 instrucciones reales se repite 8Ã—, pero con 20 y split 20% se obtiene ~17 train/3 eval). *Subirlo* (200) aÃ±ade mÃ¡s iteraciones; *bajarlo* (120) para datasets mÃ¡s variados o smoke-tests muy rÃ¡pidos.
- `PER_DEVICE_BATCH_SIZE = 1` â†’ muestras procesadas por GPU antes de acumular gradientes. Consume ~1â€¯GB y ofrece actualizaciones mÃ¡s frecuentes (1Ã—8). *Subirlo* (2) si la GPU lo permite; *bajarlo* no es posible (mÃ­nimo 1).
- `GRADIENT_ACCUMULATION = 8` â†’ nÃºmero de pasos antes de aplicar actualizaciÃ³n (batch efectivo = 1Ã—8 = 8). *Subirlo* (10) para aÃºn mÃ¡s regularizaciÃ³n; *bajarlo* (4) si necesitas converger mÃ¡s rÃ¡pido.
- `NUM_EPOCHS = 18` â†’ cada ejemplo se ve 18 veces tras repeticiÃ³n (~2â€¯880 muestras). *Subirlo* (22) si la pÃ©rdida sigue bajando; *bajarlo* (14) para convergencia mÃ¡s rÃ¡pida con datasets mÃ¡s ricos.
- `LEARNING_RATE = 2.5e-5` â†’ velocidad de aprendizaje base (25â€¯micro). Valores bajos evitan sobreajuste en datasets repetidos. *Subirlo* (3e-5) si la pÃ©rdida se estanca; *bajarlo* (2e-5) para mÃ¡xima estabilidad.
- `WARMUP_RATIO = 0.05` â†’ porcentaje inicial de pasos con LR creciente (primer ~170 pasos). *Subirlo* (0.1) si el LR arranca demasiado alto; *bajarlo* (0.02) para convergencia mÃ¡s rÃ¡pida.
- `LORA_DROPOUT = 0.25` â†’ regularizaciÃ³n sobre las capas adaptadas (alta). *Subirlo* (0.3) si sigue delirando; *bajarlo* (0.2) cuando veasunderfitting y quieras mÃ¡s fidelidad.
- `EVAL_SAMPLE_SIZE = 8` â†’ cantidad de ejemplos del split de validaciÃ³n usados en la evaluaciÃ³n rÃ¡pida (ahora hay mÃ¡s validaciÃ³n total con split 20%).

## ðŸ’¬ Script de Inferencia (`scripts/inference_lora.py`)
- Carga el adaptador LoRA desde `models/out-tinyllama-lora`.
- Usa decodificaciÃ³n determinista (sin muestreo) para validar fÃ¡cilmente regresiones.
- Incluye loop interactivo opcional y estadÃ­sticas de uso de GPU.

## â–¶ï¸ EjecuciÃ³n
```bash
# Entrenamiento
python scripts/finetune_lora.py

# Inferencia inicial
python scripts/inference_lora.py
```
> El entrenamiento guarda pesos LoRA en `models/out-tinyllama-lora`. Puedes fusionarlos con el modelo base usando `scripts/merge_adapter.py` si necesitas un Ãºnico checkpoint.

## ðŸ§© Estructura del Proyecto
```
finetuning-linux/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ instructions.jsonl           # Dataset versionado
â”œâ”€â”€ models/                          # Salidas de entrenamiento (ignorado en git)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ finetune_lora.py             # Entrenamiento LoRA (v1.1.1)
â”‚   â”œâ”€â”€ inference_lora.py            # Inferencia determinista (v1.0.2)
â”‚   â””â”€â”€ validate_environment.py      # Checklist opcional de diagnÃ³stico
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md (este documento)
```