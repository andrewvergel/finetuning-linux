# ===== Modelo (actual y sólido en ES/EN) =====
FT_MODEL_ID=Qwen/Qwen2.5-7B-Instruct
FT_USE_QLORA=true   # requiere: pip install bitsandbytes

# ===== Datos / Salida =====
FT_DATA_PATH=data/instructions.jsonl
FT_OUT_DIR=models/out-lora-qwen25-7b

# ===== Tamaños / batching =====
FT_DATASET_MIN_EXAMPLES=240
FT_PER_DEVICE_BATCH_SIZE=4      # subimos porque QLoRA 4-bit ahorra VRAM
FT_GRADIENT_ACCUMULATION=8
FT_MAX_SEQ_LEN=4096             # Qwen2.5 soporta contextos largos; el script usa el mínimo seguro

# ===== Entrenamiento =====
FT_NUM_EPOCHS=8
FT_LEARNING_RATE=1e-4
FT_WARMUP_RATIO=0.15
FT_LR_SCHEDULER=cosine
FT_WEIGHT_DECAY=0.01
FT_EVAL_STEPS=100
FT_SAVE_STEPS=100

# ===== LoRA (heads + MLP) =====
FT_LORA_RANK=16
FT_LORA_ALPHA=32
FT_LORA_DROPOUT=0.15
# Targets típicos para arquitecturas tipo Llama/Qwen/Mistral
FT_LORA_TARGET_MODULES=q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj

# ===== Misceláneos =====
FT_LOGGING_STEPS=10
FT_SAVE_STRATEGY=steps
FT_SAVE_TOTAL_LIMIT=2
FT_DATASET_SHUFFLE_SEED=42
FT_VALIDATION_SPLIT=0.2
FT_DEBUG_LOG_FILE=debug_last_run.log
FT_EVAL_MAX_NEW_TOKENS=220
FT_EVAL_SAMPLE_SIZE=10
FT_FORCE_PACKING=true

# ===== Ruido / performance =====
TOKENIZERS_PARALLELISM=false
HF_DATASETS_DISABLE_MULTIPROCESSING=1