# ===== Modelo (actual y sólido en ES/EN) =====
FT_MODEL_ID=Qwen/Qwen2.5-7B-Instruct
FT_USE_QLORA=true   # requiere: pip install bitsandbytes

# ===== Datos / Salida =====
FT_DATA_PATH=data/instructions.jsonl
FT_OUT_DIR=models/out-lora-qwen25-7b

# ===== Tamaños / batching =====
FT_DATASET_MIN_EXAMPLES=240
FT_PER_DEVICE_BATCH_SIZE=1      # Reducido para ahorrar memoria
FT_GRADIENT_ACCUMULATION=16     # Aumentado para mantener batch efectivo
FT_MAX_SEQ_LEN=1024             # Reducido para ahorrar memoria

# ===== Entrenamiento =====
FT_NUM_EPOCHS=8
FT_LEARNING_RATE=2e-4           # Ajustado para mejor convergencia
FT_WARMUP_RATIO=0.1             # Reducido warmup
FT_LR_SCHEDULER=cosine
FT_WEIGHT_DECAY=0.01
FT_EVAL_STEPS=50                # Evaluación más frecuente
FT_SAVE_STEPS=50                # Guardado más frecuente

# ===== LoRA (solo q_proj y v_proj para ahorrar memoria) =====
FT_LORA_RANK=8                  # Reducido rank
FT_LORA_ALPHA=16                # Reducido alpha
FT_LORA_DROPOUT=0.05            # Reducido dropout
# Solo q_proj y v_proj para ahorrar memoria
FT_LORA_TARGET_MODULES=q_proj,v_proj

# ===== Misceláneos =====
FT_LOGGING_STEPS=10
FT_SAVE_STRATEGY=steps
FT_SAVE_TOTAL_LIMIT=2
FT_DATASET_SHUFFLE_SEED=42
FT_VALIDATION_SPLIT=0.2
FT_DEBUG_LOG_FILE=debug_last_run.log
FT_EVAL_MAX_NEW_TOKENS=128      # Reducido para evaluación
FT_EVAL_SAMPLE_SIZE=5           # Muestras reducidas para evaluación
FT_FORCE_PACKING=false          # Desactivado para ahorrar memoria
FT_TRUST_REMOTE_CODE=1

# ===== Ruido / performance =====
TOKENIZERS_PARALLELISM=false
HF_DATASETS_DISABLE_MULTIPROCESSING=1