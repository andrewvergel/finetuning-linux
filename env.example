# ===== Modelo (actual y sólido en ES/EN) =====
FT_MODEL_ID=Qwen/Qwen2.5-7B-Instruct
FT_USE_QLORA=true   # requiere: pip install bitsandbytes

# ===== Datos / Salida =====
FT_DATA_PATH=data/instructions.jsonl
FT_OUT_DIR=models/out-lora-qwen25-7b

# ===== Tamaños / batching =====
FT_DATASET_MIN_EXAMPLES=240
FT_PER_DEVICE_BATCH_SIZE=1      # Reducido para ahorrar memoria
FT_GRADIENT_ACCUMULATION=32     # Aumentado para mantener batch efectivo
FT_MAX_SEQ_LEN=512              # Reducido significativamente para ahorrar memoria

# ===== Entrenamiento =====
FT_NUM_EPOCHS=8
FT_LEARNING_RATE=2e-4           # Ajustado para mejor convergencia
FT_WARMUP_RATIO=0.1             # Reducido warmup
FT_LR_SCHEDULER=cosine
FT_WEIGHT_DECAY=0.01
FT_EVAL_STEPS=50                # Evaluación más frecuente
FT_SAVE_STEPS=50                # Guardado más frecuente

# ===== LoRA (solo q_proj para ahorrar memoria) =====
FT_LORA_RANK=4                  # Rank reducido al mínimo
FT_LORA_ALPHA=8                 # Alpha reducido
FT_LORA_DROPOUT=0.01            # Dropout mínimo
# Solo q_proj para ahorrar memoria (capa más importante)
FT_LORA_TARGET_MODULES=q_proj

# ===== Misceláneos =====
FT_LOGGING_STEPS=10
FT_SAVE_STRATEGY=steps
FT_SAVE_TOTAL_LIMIT=2
FT_DATASET_SHUFFLE_SEED=42
FT_VALIDATION_SPLIT=0.2
FT_DEBUG_LOG_FILE=debug_last_run.log
FT_EVAL_MAX_NEW_TOKENS=128      # Reducido para evaluación
FT_EVAL_SAMPLE_SIZE=5           # Muestras reducidas para evaluación
FT_FORCE_PACKING=false          # Desactivado para ahorrar memoria
FT_TRUST_REMOTE_CODE=1

# ===== Optimizaciones de memoria =====
# Configuración de PyTorch para manejo de memoria
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True

# ===== Rendimiento =====
# Deshabilitar paralelismo de tokenizers para evitar deadlocks
TOKENIZERS_PARALLELISM=false
# Deshabilitar multiprocesamiento de datasets
HF_DATASETS_DISABLE_MULTIPROCESSING=1