FT_MODEL_ID=microsoft/DialoGPT-medium
FT_DATA_PATH=data/instructions.jsonl
FT_OUT_DIR=models/out-tinyllama-lora

# Dataset expansion & batching
FT_DATASET_MIN_EXAMPLES=240
FT_PER_DEVICE_BATCH_SIZE=1
FT_GRADIENT_ACCUMULATION=12

# Training schedule
FT_NUM_EPOCHS=12
FT_LEARNING_RATE=2e-5
FT_WARMUP_RATIO=0.1
FT_LR_SCHEDULER=linear
FT_WEIGHT_DECAY=0.1

# LoRA configuration
FT_LORA_RANK=32
FT_LORA_ALPHA=32
FT_LORA_DROPOUT=0.3
FT_LORA_TARGET_MODULES=c_attn,c_proj

# Misc
FT_LOGGING_STEPS=5
FT_SAVE_STRATEGY=epoch
FT_SAVE_TOTAL_LIMIT=3
FT_DATASET_SHUFFLE_SEED=42
FT_VALIDATION_SPLIT=0.2
FT_DEBUG_LOG_FILE=debug_last_run.log
FT_EVAL_MAX_NEW_TOKENS=220
FT_EVAL_SAMPLE_SIZE=10
