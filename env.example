# ===== Model =====
FT_MODEL_ID=TinyLlama/TinyLlama-1.1B-Chat-v1.0
FT_USE_QLORA=true

# ===== Data / Output =====
FT_DATA_PATH=data/instructions.jsonl
FT_OUT_DIR=models/out-lora-qwen25-7b

# ===== Batch Sizes =====
FT_DATASET_MIN_EXAMPLES=240
FT_PER_DEVICE_BATCH_SIZE=1      # Minimal batch size
FT_GRADIENT_ACCUMULATION=64     # Increased to maintain effective batch size
FT_MAX_SEQ_LEN=64               # Drastically reduced to 64 tokens

# ===== Training =====
FT_NUM_EPOCHS=10                # Slightly more epochs due to smaller batch
FT_LEARNING_RATE=1e-4           # Reduced learning rate
FT_WARMUP_RATIO=0.05            # Minimal warmup
FT_LR_SCHEDULER=cosine
FT_WEIGHT_DECAY=0.01
FT_EVAL_STEPS=50
FT_SAVE_STEPS=50

# ===== LoRA (Minimal Configuration) =====
FT_LORA_RANK=1                  # Minimal rank
FT_LORA_ALPHA=2                 # Minimal alpha
FT_LORA_DROPOUT=0.0             # No dropout
# Only target q_proj (most important layer)
FT_LORA_TARGET_MODULES=q_proj

# ===== Memory Optimizations =====
# PyTorch memory management
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:16,expandable_segments:True

# ===== Performance =====
# Disable parallelism to avoid deadlocks
TOKENIZERS_PARALLELISM=false
HF_DATASETS_DISABLE_MULTIPROCESSING=1
OMP_NUM_THREADS=1
MKL_NUM_THREADS=1

# Additional optimizations
PIPELINE_PARALLELISM=0
MODEL_PARALLELISM=0
GRADIENT_CHECKPOINTING=true
USE_GRADIENT_CHECKPOINTING=true